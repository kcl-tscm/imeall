import os
import sys
import json
import glob
import logging
import argparse
import numpy as np

from peewee   import *
from datetime import datetime, timedelta
from models   import GBAnalysis, PotentialParameters
from quippy   import Atoms, set_fortran_indexing, Potential, AtomsReader

set_fortran_indexing(False)
GRAIN_DATABASE = "/home/lambert/pymodules/imeall/imeall/grain_boundaries/"

logging.basicConfig(filename='example.log',level=logging.DEBUG)

try: 
  DATABASE   = os.environ['GBDATABASE']
except KeyError:
  sys.exit("NO SQL GBDATABASE in Environment")

database       = SqliteDatabase(DATABASE)
class BaseModel(Model):
  class Meta():
    database = database

class GrainBoundary(BaseModel):
  """
  Canonical Parent Grain Model.
  Vectors are serialized to csv.
  Parameters:
  path: relative to the grain boundary database root.
  material: name for the material class (e.g. alphaFe)
  """
#add material
# material         = CharField()
  orientation_axis = CharField()
  angle            = FloatField()
  boundary_plane   = CharField()
  path             = CharField()
  gb_type          = CharField()
  z_planes         = CharField()
  n_at             = IntegerField()
  coincident_sites = IntegerField()
  sigma_csl        = IntegerField(default=0)
  height           = FloatField()
  area             = FloatField()
  notes            = TextField(default="")
  gbid             = CharField(unique=True)

class SubGrainBoundary(BaseModel):
  """
  SubGrainBoundary Model
  path: relative to the grainboundary database root.
  params: rbt rigid body translations.
  params: grain_boundary every grain is a subgrain of the GrainBoundary Class.
  """
  path            = CharField()
  canonical_grain = ForeignKeyField(GrainBoundary, "subgrains")
  converged       = BooleanField()
  rbt             = CharField()
  potential       = CharField()
  rcut            = FloatField()
  area            = FloatField()
  n_at            = IntegerField()
  E_gb            = FloatField(default=0.0)
  E_gb_init       = FloatField(default=0.0)
  notes           = TextField(default="")
  gbid            = CharField()
  #create integrity index so that we can't have two entries for same potential and gbid
  class Meta:
		indexes=(
     				  (('potential', 'gbid'), True), #trailing comma is necessary
    				)

class Fracture(BaseModel):
  """
  Fracture Simulation Model
  params: G stress energy release rate.
  params: strain_rate.
  params: sim_T simulation temperature.
  """
  fracture_system = CharField()
  G               = FloatField()
  strain_rate     = FloatField()
  sim_T           = FloatField()
  notes           = CharField(default="")

class Dislocation(BaseModel):
  gbid  =  CharField()

def serialize_vector(vector):
  return ','.join(map(str, vector))

def deserialize_vector_float(ser_vec):
  return map(float, ser_vec.split(','))

def deserialize_vector_int(ser_vec):
  return map(int, ser_vec.split(','))

def create_tables(database):
  """
  :method:`create_tables` 
  """
  database.connect()
  database.create_tables([GrainBoundary,SubGrainBoundary], True)

def add_conv_key(material='alphaFe', or_axis='001'):
  """
  :method:`add_conv_key` check if subgb.json directory contains a convergence
  key. If not add key to subgb.json file and default to false.
  """
  analyze  = GBAnalysis()
  gb_files = []
  analyze.find_gb_json('{0}'.format(os.path.join(GRAIN_DATABASE, os.path.join(material, or_axis))), gb_files, 'gb.json')
  for gb in gb_files:
    print gb[0], gb[1]
    with open(gb[1], 'r') as f:
      gb_json = json.load(f)
    GB_model = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
    for subgb_model in GB_model.subgrains:
      subgb_dict_path = os.path.join(subgb_model.path,'subgb.json')
      subgb_dict_path = os.path.join(GRAIN_DATABASE, subgb_dict_path)
      with open(subgb_dict_path,'r') as f:
        subgb_dict = json.load(f)
      try:
        print subgb_dict['gbid'], subgb_dict['converged']
      except KeyError:
        print 'Adding Convergence Keys'
        subgb_dict['converged'] = False
        with open(subgb_dict_path,'w') as f:
          json.dump(subgb_dict, f, indent=2)

def gb_check_dir_integrity(material='alphaFe', or_axis='001'):
  """
  :method:`gb_check_dir_integrity` check if directory in directory tree contains 
  a grain boundary json file, and update sql database to include the parent grain
  if it is missing.
  """
  analyze  = GBAnalysis()
  gb_files = []
  analyze.find_gb_json('{0}'.format(os.path.join(GRAIN_DATABASE, os.path.join(material, or_axis))), gb_files, 'gb.json')
  for gb in gb_files:
    with open(gb[1], 'r') as f:
      gb_json = json.load(f)
    GB_model = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
    for subgb_model in GB_model.subgrains:
      subgb_dict_path = os.path.join(subgb_model.path,'subgb.json')
      subgb_dict_path = os.path.join(GRAIN_DATABASE, subgb_dict_path)
      try:
        with open(subgb_dict_path,'r') as f:
          subgb_dict = json.load(f)
      except IOError:
        NOINPUT = True
        print subgb_dict_path
        while NOINPUT:
          user_input = raw_input("Directory missing delete model (y/n)?")
          if user_input == 'y':
            print 'Deleting Model'
            subgb_model.delete_instance()
            NOINPUT=False
          elif user_input =='n':
            print 'Keeping Model'
            NOINPUT=False
          else:
            pass

def gb_check_path(material='alphaFe', or_axis='001', modify_db=False):
  """
  :method:`gb_check_path` compare consistency between location of subgb.json
  paths and the paths in the closure tree.
  """
  analyze  = GBAnalysis()
  gb_files = []
  analyze.find_gb_json('{0}'.format(os.path.join(GRAIN_DATABASE, os.path.join(material, or_axis))), gb_files, 'gb.json')
  no_struct_file = open('no_struct.txt','a')
  for gb_num, gb in enumerate(gb_files[:]):
    with open(gb[1], 'r') as f:
      gb_json = json.load(f)
    GB_model = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
    json_path = '/'.join(gb[0].split('/')[7:])
    #check grain model has correct path!
    try:
      assert json_path == GB_model.path
    except AssertionError:
      print GB_model.path, json_path
      q = GrainBoundary.update(path=json_path).where(GrainBoundary.gbid==gb_json['gbid'])
      q.execute()
    #now pull subgb.json paths 
    subgb_files = []
    analyze.find_gb_json('{0}'.format(gb[0]), subgb_files, 'subgb.json')
    for subgb in subgb_files:
      subgb_dict_path = os.path.join(subgb[0],'subgb.json')
      subgb_dict_path = os.path.join(GRAIN_DATABASE, subgb_dict_path)
      with open(subgb_dict_path,'r') as f:
        subgb_dict = json.load(f)

      print subgb_dict_path
      query = (GB_model.subgrains
                       .where((SubGrainBoundary.gbid == subgb_dict['name']) & (SubGrainBoundary.potential==subgb_dict['param_file'])))
      subgb_model = query.get()
      json_path  = '/'.join(subgb[0].split('/')[7:])
      model_path  = subgb_model.path
      try:
        assert json_path == model_path
      except AssertionError:
        #print subgb_dict['name'], subgb_model.gbid
        print json_path, model_path
        query = (SubGrainBoundary.update(path=json_path)
                                 .where((SubGrainBoundary.gbid == subgb_dict['name']) &
                                        (SubGrainBoundary.potential==subgb_dict['param_file'])))
        #print query
        query.execute()
    database.commit()
  return

def gb_check_conv(material='alphaFe', or_axis='001', modify_db=False):
  """
  :method:`gb_check_conv` scans through grainboundary directory tree,
           inspecting the json dictionary and update the SQLite model if 
           the grain boundary energy or convergence flag is inconsistent.
  :attributes:
  material: Which material to do check json/database convergence consistency on.
  or_axis: Which orientation axis to check.
  modify_db: Boolean. If True updates gb_model in database otherwise just prints inconsistent grain json/database value.
  """
  analyze  = GBAnalysis()
  gb_files = []
  analyze.find_gb_json('{0}'.format(os.path.join(GRAIN_DATABASE, os.path.join(material, or_axis))), gb_files, 'gb.json')
  no_struct_file = open('no_struct.txt','a')
  for gb_num, gb in enumerate(gb_files[:]):
    with open(gb[1], 'r') as f:
      gb_json = json.load(f)
    GB_model = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
    for subgb_model in GB_model.subgrains:
      subgb_dict_path = os.path.join(subgb_model.path,'subgb.json')
      subgb_dict_path = os.path.join(GRAIN_DATABASE, subgb_dict_path)
      with open(subgb_dict_path,'r') as f:
        subgb_dict = json.load(f)
      struct_path = os.path.join(subgb_model.path, subgb_model.gbid+'_traj.xyz')
      struct_path = os.path.join(GRAIN_DATABASE, struct_path)
      try:
        logging.debug(struct_path)
        assert subgb_model.converged==subgb_dict['converged']
      except AssertionError:
        if not modify_db:
          print 'Not updating:'
          print subgb_dict_path
          print 'Model: ', subgb_model.converged, 'Json:', subgb_dict['converged']
        else:
          try:
            assert type(subgb_dict['converged'])==bool  
          except:
            print "json 'converged' value not boolean. json file could be corrupted:"
            print subgb_dict_path
          else:
            print 'Updating model instance in database:'
            print subgb_dict_path
            print 'Model: ', subgb_model.converged, 'Json:', subgb_dict['converged']
            subgb_model.converged = subgb_dict['converged']
            subgb_model.save()
      try:
        assert (abs(subgb_model.E_gb - subgb_dict['E_gb']) < 1e-8)
      except AssertionError:
        if not modify_db:
          print 'Not updating:'
          print 'Model E_gb:', subgb_model.E_gb, 'JSON E_gb:',  subgb_dict['E_gb']
        else:
          print 'Model E_gb:', subgb_model.E_gb, 'JSON E_gb:',  subgb_dict['E_gb']
          print subgb_dict_path
          subgb_model.E_gb = subgb_dict['E_gb']
          subgb_model.save()
      except KeyError:
        subgb_dict['converged']=False
        subgb_dict['E_gb'] = 0.0
        with open(subgb_dict_path, 'w') as f:
          json.dump(subgb_dict, f, indent=2)

def gb_check_force(material='alphaFe', or_axis='001', force_tol=0.05, modify_db=False):
  """
  :method:`gb_check_force`. Recurse through directory tree, loading the structure file, json dict 
  and the model for each subgrain. Check that the force tolerance in the structure file has actually been 
  met for convergence.
  """
  analyze  = GBAnalysis()
  gb_files = []
  analyze.find_gb_json('{0}'.format(os.path.join(GRAIN_DATABASE, os.path.join(material, or_axis))), gb_files, 'gb.json')
  start     = 6
  sub_start = 0
  no_struct_file = open('no_struct.txt','a')
  for gb_num, gb in enumerate(gb_files[start:]):
    with open(gb[1], 'r') as f:
      gb_json = json.load(f)
    GB_model = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
    for subgb_num, subgb_model in enumerate(GB_model.subgrains[sub_start:]):
      subgb_dict_path = os.path.join(subgb_model.path,'subgb.json')
      subgb_dict_path = os.path.join(GRAIN_DATABASE, subgb_dict_path)
      with open(subgb_dict_path,'r') as f:
        subgb_dict = json.load(f)
      struct_path = os.path.join(subgb_model.path, subgb_model.gbid+'_traj.xyz')
      struct_path = os.path.join(GRAIN_DATABASE, struct_path)
      try:
        ats = AtomsReader(struct_path)[-1]
      except RuntimeError:
        print 'No Struct File'
      except EOFError:
        print 'Struct File corrupted'
      except IOError:
        print 'No Traj File'
      else:  
        print gb_num+start, subgb_num+sub_start, struct_path
        try:
          forces = [np.sqrt(x**2+y**2+z**2) for x,y,z, in zip(ats.properties['force'][0], ats.properties['force'][1], ats.properties['force'][2])]
        except KeyError:
          print gb_num+start, struct_path
          print 'No Force in atoms object'
          conv_check = False
        else:
          if max(forces) <= force_tol:
            conv_check = True
          else:
            conv_check = False
            subgb_dict['E_gb'] = 0.0

      if modify_db:
        subgb_dict['converged'] = conv_check
        print struct_path
        print subgb_dict['converged'], conv_check
        with open(subgb_dict_path, 'w') as f:
          json.dump(subgb_dict, f, indent=2)
      else:
        print struct_path
        try:
          print 'JSON', subgb_dict['converged'], conv_check
        except KeyError:
          print 'no convergence key'
          subgb_dict['converged']=conv_check
          with open(subgb_dict_path, 'w') as f:
            json.dump(subgb_dict, f, indent=2)

def change_json_key(material='alphaFe', or_axis='001'):
  """
  :method:`change_json_key` replaces '_traj' string in subgb_dict['gbid']
  method could be useful if we wish to change database/subgb strings.
  """
  analyze  = GBAnalysis()
  gb_files = []
  analyze.find_gb_json('{0}'.format(os.path.join(GRAIN_DATABASE, os.path.join(material, or_axis))), gb_files, 'gb.json')
  for gb in gb_files:
    with open(gb[1], 'r') as f:
      gb_json = json.load(f)
    GB_model = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
    for subgb_model in GB_model.subgrains:
      subgb_dict_path = os.path.join(subgb_model.path,'subgb.json')
      subgb_dict_path = os.path.join(GRAIN_DATABASE, subgb_dict_path)
      with open(subgb_dict_path,'r') as f:
        subgb_dict = json.load(f)
      try:
        assert subgb_dict['gbid'] == subgb_model.gbid
        if '_traj' in subgb_dict['gbid']:
          print subgb_dict['gbid'], subgb_model.gbid
          subgb_dict['gbid']= subgb_dict['gbid'].replace('_traj','')
          with open(subgb_dict_path,'w') as f:
            json.dump(subgb_dict, f, indent=2)
      except AssertionError:
        print subgb_dict['gbid'], subgb_model.gbid
        subgb_model.gbid = subgb_dict['gbid']
        subgb_model.save()
        print subgb_model.gbid

def insert_subgrain(material='alphaFe', or_axis='110', gbid='1108397110', json_path='/'):
  GB_model = GrainBoundary.select().where(GrainBoundary.gbid==gbid).get()
  with open(os.path.join(GRAIN_DATABASE, json_path)+'/subgb.json','r') as f:
    json_dict = json.load(f)
  model_vars = ['canonical_grain', 'converged', 'E_gb_init', 'potential', 'rbt', 'path', 'area', 'rcut', 'n_at', 'E_gb', 'notes', 'gbid']

  converged = json_dict['converged']
  E_gb_init = json_dict['E_gb_init']
  potential = json_dict["param_file"]
  rbt = serialize_vector(json_dict['rbt'])
  area = json_dict["rcut"]
  rcut = json_dict["area"]
  n_at = json_dict['n_at']
  E_gb = json_dict['E_gb']
  sub_gbid  = json_dict['gbid']

  subgb_dict = {"canonical_grain"  : GB_model,
                "converged"        : converged,
                "E_gb_init"        : E_gb_init, 
                "potential"        : potential,
                "rbt"              : rbt, 
                "path"             : json_path,
                "area"             : area,
                "rcut"             : rcut,
                "n_at"             : n_at,
                "E_gb"             : E_gb,
                "notes"            : "",
                "gbid"             : sub_gbid
              }
  #try:
  print GB_model.gbid
  print subgb_dict 
  print SubGrainBoundary.create(**subgb_dict)        
  logging.info('Created entry {}'.format(subgb_dict))
  #except IntegrityError:
  #  logging.info('GB already in DB {}'.format(subgb_dict))

def populate_db(material='alphaFe', or_axis='001', gbid='001804711130', modify=False):
  """
  method:`populate_db` add canonical grains to SQLite database.
  """
  analyze  = GBAnalysis()
  if len(gbid) == 0:
    dir_str  = os.path.join(material, or_axis)
  else:
    #pull specific grainboundary
    dir_str  = os.path.join(material, or_axis)
    dir_str  = os.path.join(dir_str, gbid)
  logging.info('dir_str {}'.format(dir_str))
  gb_files = []
  analyze.find_gb_json('{0}'.format(os.path.join(GRAIN_DATABASE, dir_str)), gb_files, 'gb.json')
  for gb in gb_files:
    print gb[0], gb[1]
    with open(gb[1], 'r') as f:
      gb_json = json.load(f)
    print gb_json
    try:
      sigma_csl = gb_json['sigma_csl']
    except KeyError:
      sigma_csl = int(gb_json['n_at']/(gb_json['coincident_sites']+gb_json['n_at']))
      gb_json['sigma_csl'] = sigma_csl
      with open(gb[1], 'w') as f:
        json.dump(gb_json, f, indent=2)

    try:
      coincident_sites = gb_json['coincident_sites']
    except KeyError:
      coincident_sites = 0

    gb_dict = {"gb_type"          : gb_json['type'],
               "n_at"             : gb_json['n_at'],
               "boundary_plane"   : serialize_vector(map(int, gb_json['boundary_plane'])),
               "orientation_axis" : serialize_vector(map(int, gb_json['orientation_axis'])),
               "z_planes"         : serialize_vector(gb_json['zplanes']),
               "coincident_sites" : coincident_sites,
               "sigma_csl"        : sigma_csl,
               "angle"            : gb_json['angle'],
               "height"           : gb_json['H'],
               "area"             : gb_json['A'],
               "notes"            : "",
               "path"             : os.path.relpath(gb[0], "/home/lambert/pymodules/imeall/imeall/grain_boundaries/"),
               "gbid"             : gb_json['gbid']
              }

    if modify:
      try:
        GB_model_object = GrainBoundary.create(**gb_dict)
      except IntegrityError:
        GB_model_object = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
        print 'GB already in database'
    else:
      GB_model_object = GrainBoundary.select().where(GrainBoundary.gbid==gb_json['gbid']).get()
      print GB_model_object

    subgb_files = []
    analyze.find_gb_json('{0}'.format(gb[0]), subgb_files, 'subgb.json')
    for subgb in subgb_files:
      with open(subgb[1],'r') as f:
        subgb_json = json.load(f)
      try: 
        converged = subgb_json['converged']
      except KeyError:
        converged = False
      try:
        E_gb = subgb_json["E_gb"]
      except KeyError:
        E_gb = 0.0
      try:
        E_gb_init=subgb_json["E_gb_init"]
      except KeyError:
        E_gb_init = 0.0
      try:
        gbid = subgb_json["gbid"]
      except KeyError:
        gbid = subgb_json["name"]
      try:
        area = subgb_json['A']
      except KeyError:
        structs = glob.glob(os.path.join(subgb[0], '*.xyz'))
        struct  = Atoms(structs[-1])
        cell    = struct.get_cell()
        area    = cell[0][0]*cell[1][1]
        subgb_json['n_at'] = len(struct)
        
      subgb_dict = {"canonical_grain" : GB_model_object,
                    "converged"       : converged,
                    "E_gb_init"       : E_gb_init, 
                    "potential"       : subgb_json["param_file"],
                    "rbt"             : serialize_vector(subgb_json['rbt']),
                    "path"            : os.path.relpath(subgb[0], "/home/lambert/pymodules/imeall/imeall/grain_boundaries/"),
                    "area"            : area,
                    "rcut"            : subgb_json["rcut"],
                    "n_at"            : subgb_json['n_at'],
                    "E_gb"            : E_gb,
                    "notes"           : "",
                    "gbid"            : gbid}
      try:
        SubGrainBoundary.create(**subgb_dict)        
        logging.info('Created entry {}'.format(subgb_dict))
      except IntegrityError:
        logging.info('GB already in DB {}'.format(subgb_dict))

if __name__=="__main__":
  parser = argparse.ArgumentParser()
#Specifieri
  parser.add_argument("-m","--material", help="The material we wish to query.", default="alphaFe")
  parser.add_argument("-o","--or_axis", help="Orientation axis to pull from database. (default: 001)", default="001")
  parser.add_argument("-gbt","--gb_type", help="Type of grain boundary to pull: mixed, tilt, twist. (default: tilt)", default="tilt")
  parser.add_argument("-pt","--potential", help="potential to pull from database. Use --list_potential to see available potentials.", default="PotBH.xml")
  parser.add_argument("-gbid","--gbid", help="grain boundary id if present only perform action on specific boundary. applies to populate.", default="")
  parser.add_argument("-j","--json_path", help="Path (relative to Database root of subgrain to be added to database).", default="")
  parser.add_argument("-mod","--modify", help="Generic flag. If included database will be updated, otherwise program just reports intended actions  \
                                                  without  modifying database. Applies to check_conv and check_force.", action="store_true")
#Actions
  parser.add_argument("-l","--list", help="List converged structures in database and their energies.", action="store_true")
  parser.add_argument("-a","--populate", help="Recurse through directory tree add subgrains to SQLite database. Choose orientation with or_axis.", action="store_true")
  parser.add_argument("-i","--insert", help="Add SubGrainboundary to database: requires gbid, or_axis, relative path to json_file.", action="store_true")
  parser.add_argument("-p","--prune", help="Remove structures from SQL database that are no longer in directory tree.", action="store_true")
  parser.add_argument("-cc","--check_conv", help="Check that convergence status of database json files corresponds to SQL database.", action="store_true")
  parser.add_argument("-cp","--check_path", help="Check that path of database json files corresponds to SQL database.", action="store_true")
  parser.add_argument("-cf","--check_force", help="Inspect xyz structure files to determine if force convergence has been reached.", action="store_true")
  parser.add_argument("-lp","--list_potential", help="List potentials in database", action="store_true")
  parser.add_argument("-la","--list_all", help="List all Grain in database along with path", action="store_true")

  args   = parser.parse_args()
  database.connect()
  if args.list:
    oraxis = '0,0,1'
    pot_param     = PotentialParameters()
    ener_per_atom = pot_param.gs_ener_per_atom()

    if args.gb_type=='tilt':
      selected_grains = GrainBoundary.select().where(GrainBoundary.orientation_axis==oraxis).where(GrainBoundary.boundary_plane != oraxis)
    elif args.gb_type=='twist':
      selected_grains = GrainBoundary.select().where(GrainBoundary.orientation_axis==oraxis).where(GrainBoundary.boundary_plane == oraxis)
    else:
      sys.exit('Invalid grain boundary type.')

    print 'Material: {}, Orientation Axis: {} Potential: {}'.format(args.material, ' '.join(args.or_axis.split(',')), args.potential)
    for gb in selected_grains.order_by(GrainBoundary.angle):
      subgbs = (gb.subgrains.select(GrainBoundary, SubGrainBoundary)
                  .where(SubGrainBoundary.potential==args.potential)
                  .join(GrainBoundary).dicts())
  
      if len(subgbs) > 0:
        subgbs = [(16.02*(subgb['E_gb']-float(subgb['n_at']*ener_per_atom[args.potential]))/(2.0*subgb['area']), subgb) for subgb in subgbs]
        subgbs.sort(key = lambda x: x[0])
        print '\t {} {} {}'.format(round(gb.angle*(180.0/3.14159),3), round(subgbs[0][0],3), subgbs[0][1]['path'])

  if args.list_all:
    for gb in GrainBoundary.select().order_by(GrainBoundary.orientation_axis).order_by(GrainBoundary.angle):
      print gb.orientation_axis, gb.angle*(180/np.pi), gb.gb_type, gb.path

    for gb in GrainBoundary.select().order_by(GrainBoundary.orientation_axis).order_by(GrainBoundary.angle):
      for subgb in gb.subgrains:
        print gb.orientation_axis, gb.angle*(180/np.pi), subgb.potential, subgb.path

  if args.list_potential:
    pot_param     = PotentialParameters()
    ener_per_atom = pot_param.gs_ener_per_atom()
    for k, v in ener_per_atom.items():
      print 'Potential Name: {}, Per Atom Energy: {}'.format(k, v)

  if args.populate:
    populate_db(material=args.material, or_axis=args.or_axis, modify=args.modify, gbid=args.gbid)

  if args.prune:
    gb_check_dir_integrity(material=args.material, or_axis=args.or_axis)

  if args.check_conv:
    gb_check_conv(material=args.material, or_axis=args.or_axis, modify_db=args.modify)

  if args.check_force:
    gb_check_force(material=args.material, or_axis=args.or_axis, modify_db=args.modify)

  if args.insert:
    assert args.gbid != ''
    assert args.json_path != ''
    insert_subgrain(material=args.material, or_axis=args.or_axis, gbid=args.gbid, json_path=args.json_path)

  if args.check_path:
    gb_check_path(material=args.material, or_axis=args.or_axis, modify_db=args.modify)
  database.close()
